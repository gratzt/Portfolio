[
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Trevor Gratz",
    "section": "",
    "text": "Senior data scientist with over a decade of experience in public sector analytics, statistical modeling, and data system design. Proven success in interdisciplinary collaboration, publication, and implementing production-grade tools to support government decision-making.\n\n\nSeattle, Washington\n(206) 948-2815\ngithub.com/gratzt\n\n\n\n\n\n\nPierce County Finance, Research and Data Analysis | Tacoma, WA | March 2022 – Present\n\n\nLed interdisciplinary efforts with Human Services, IT Engineering, Legal, and external providers to design new data collection systems supporting $30 million in contracted behavioral health services\nDeveloped a geospatial unsupervised clustering algorithm for opioid treatment location planning; built an interactive data exploration portal to support project management\nIdeated and scoped projects to determine appropriate inferential statistical methods to drive decision-making for key stakeholders, including the County Executive, Director of Finance, Director of Human Services, Director of Planning and Public Works, and Prosecuting Attorney\nMentored analysts on code structure, distributed workflows, and version control; guided Git adoption across the team\n\n\n\n\n\nUniversity of Washington, Center for Education Data and Research | Seattle, WA | June 2018 – December 2021\n\n\nIntegrated health and education datasets using probabilistic record linkage; co-authored the first peer-reviewed study of Washington’s school nurse workforce\nTranslated ambiguous research goals into published findings, including a fixed-effects Poisson model analyzing online education enrollment trends\nConducted analysis of a randomized controlled trial using randomized block design to assess student teaching outcomes\n\n\n\n\n\nUniversity of Washington, Center for Education Data and Research | Seattle, WA | September 2014 – May 2018\n\n\nManaged administrative and ad hoc datasets; contributed to over 10 peer-reviewed publications with 400+ citations\nApplied causal inference methods (e.g., difference-in-differences) to evaluate the impact of financial aid programs, resulting in three peer-reviewed publications\n\n\n\n\n\n\n\n\nLanguages & Tools\nPython (pandas, scikit-learn, PyTorch, statsmodels, matplotlib), R, SQL, Stata, Git\n\n\nTechniques\nRegression, Classification, Deep Learning, NLP, Clustering, Causal Inference, Geospatial Analysis, EDA, Model Evaluation\n\n\nFrameworks\nSupervised & Unsupervised Learning, Distributed Workflows, Reproducible Research\n\n\n\n\n\n\n\nGeorgia Institute of Technology | Atlanta, GA | Expected August 2025\n\n\n\nPacific Lutheran University | Tacoma, WA | August 2019\n\n\n\nUniversity of Washington | Seattle, WA | March 2014"
  },
  {
    "objectID": "Resume.html#professional-summary",
    "href": "Resume.html#professional-summary",
    "title": "Trevor Gratz",
    "section": "",
    "text": "Senior data scientist with over a decade of experience in public sector analytics, statistical modeling, and data system design. Proven success in interdisciplinary collaboration, publication, and implementing production-grade tools to support government decision-making."
  },
  {
    "objectID": "Resume.html#experience",
    "href": "Resume.html#experience",
    "title": "Trevor Gratz",
    "section": "",
    "text": "Pierce County Finance, Research and Data Analysis | Tacoma, WA | March 2022 – Present\n\n\nLed interdisciplinary efforts with Human Services, IT Engineering, Legal, and external providers to design new data collection systems supporting $30 million in contracted behavioral health services\nDeveloped a geospatial unsupervised clustering algorithm for opioid treatment location planning; built an interactive data exploration portal to support project management\nIdeated and scoped projects to determine appropriate inferential statistical methods to drive decision-making for key stakeholders, including the County Executive, Director of Finance, Director of Human Services, Director of Planning and Public Works, and Prosecuting Attorney\nMentored analysts on code structure, distributed workflows, and version control; guided Git adoption across the team\n\n\n\n\n\nUniversity of Washington, Center for Education Data and Research | Seattle, WA | June 2018 – December 2021\n\n\nIntegrated health and education datasets using probabilistic record linkage; co-authored the first peer-reviewed study of Washington’s school nurse workforce\nTranslated ambiguous research goals into published findings, including a fixed-effects Poisson model analyzing online education enrollment trends\nConducted analysis of a randomized controlled trial using randomized block design to assess student teaching outcomes\n\n\n\n\n\nUniversity of Washington, Center for Education Data and Research | Seattle, WA | September 2014 – May 2018\n\n\nManaged administrative and ad hoc datasets; contributed to over 10 peer-reviewed publications with 400+ citations\nApplied causal inference methods (e.g., difference-in-differences) to evaluate the impact of financial aid programs, resulting in three peer-reviewed publications"
  },
  {
    "objectID": "Resume.html#skills",
    "href": "Resume.html#skills",
    "title": "Trevor Gratz",
    "section": "",
    "text": "Languages & Tools\nPython (pandas, scikit-learn, PyTorch, statsmodels, matplotlib), R, SQL, Stata, Git\n\n\nTechniques\nRegression, Classification, Deep Learning, NLP, Clustering, Causal Inference, Geospatial Analysis, EDA, Model Evaluation\n\n\nFrameworks\nSupervised & Unsupervised Learning, Distributed Workflows, Reproducible Research"
  },
  {
    "objectID": "Resume.html#education",
    "href": "Resume.html#education",
    "title": "Trevor Gratz",
    "section": "",
    "text": "Georgia Institute of Technology | Atlanta, GA | Expected August 2025\n\n\n\nPacific Lutheran University | Tacoma, WA | August 2019\n\n\n\nUniversity of Washington | Seattle, WA | March 2014"
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Publications",
    "section": "",
    "text": "Goldhaber, D., Ronfeldt, M., Cowan, J., Bardelli, E., Gratz, T., Truwit, M. (2025). “Clinical Experiences and (Unexpected Findings on) Job Placements: Experimental Evidence from Student Teaching Interventions.” CALDER Working Paper No. 312-0325 Free Working Paper Version\nGoldhaber, D. & Gratz, T. (2023). “School District Staffing Challenges in a Rapidly Recovering Economy.” Washington Educational Research Association Educational Journal-Volume 15. Free Working Paper Version\nGratz, T., Goldhaber, D., & Brown, N. (2022). “The Pre-Pandemic Growth in Online Public Education and the Factors that Predict It.” Journal of School Choice: International Research and Reform Free Working Paper Version\nGoldhaber, D., Ronfeldt, M., Cowan, J., Gratz, T., Bardelli, E., & Truwit, M. (2022). “Room for improvement? Mentor teachers and the evolution of teacher preservice clinical evaluations.” American Educational Research Journal Free Open Access Paper\nGratz, T., Goldhaber, D., Willgerodt, M., & Brown, N. (2021). “The Front-Line Health Care Workers in Schools: Health Equity, the Distribution of School Nurses, and Student Access.” The Journal of School Nursing. Free Working Paper Version\nTheobald, R., Plasman, J., Gottfried, M., Gratz, T., Holden, K., & Goldhaber, D. (2021). “Sometimes less, sometimes more: Trends in career and technical education participation for students with disabilities.” Educational Researcher. Free Working Paper Version\nLong, M., Goldhaber, D., & Gratz, T. (2020). “Washington’s College Bound Scholarship Program and its Effect on College Entry, Persistence, and Completion.” Education Finance and Policy. Free Working Paper Version\nGoldhaber, D., Long, M., Gratz, T., & Rooklyn, J. (2019). “Pledging to Do ‘Good’: An Early Commitment Pledge Program, College Scholarships, and High School Outcomes in Washington State.” Educational Evaluation and Policy Analysis. Free Working Paper Version\nGoldhaber, D., Long, M. C., Person, A. E., Rooklyn, J., & Gratz, T. (2019). “Sign Me Up: The Factors Predicting Students’ Enrollment in an Early-Commitment Scholarship Program.” AERA Open. Free Open Access Paper\nTheobald, R., Goldhaber, D., Gratz, T., & Holden, K. (2018). “High school English Language Arts teachers and postsecondary outcomes for students with and without disabilities.” Journal of Disability Policy Studies. Free Working Paper Version\nTheobald, R., Goldhaber, D., Gratz, T., & Holden, K. (2018). “Career and Technical Education, Inclusion, and Postsecondary Outcomes for Students With Learning Disabilities.” Journal of learning disabilities. Free Working Paper Version\nGoldhaber, D., Gratz, T., & Theobald, R. (2017). “What’s in a teacher test? Assessing the relationship between teacher licensure test scores and student STEM achievement and course-taking.” Economics of Education Review. Free Working Paper Version\nPfeiffer, L., & Gratz, T. (2016). “The effects of rights-based fisheries management on risk taking and fishing safety.” Proceedings of the National Academy of Sciences. Free Open Access Paper"
  },
  {
    "objectID": "Publications.html#peer-reviewed-publications",
    "href": "Publications.html#peer-reviewed-publications",
    "title": "Publications",
    "section": "",
    "text": "Goldhaber, D., Ronfeldt, M., Cowan, J., Bardelli, E., Gratz, T., Truwit, M. (2025). “Clinical Experiences and (Unexpected Findings on) Job Placements: Experimental Evidence from Student Teaching Interventions.” CALDER Working Paper No. 312-0325 Free Working Paper Version\nGoldhaber, D. & Gratz, T. (2023). “School District Staffing Challenges in a Rapidly Recovering Economy.” Washington Educational Research Association Educational Journal-Volume 15. Free Working Paper Version\nGratz, T., Goldhaber, D., & Brown, N. (2022). “The Pre-Pandemic Growth in Online Public Education and the Factors that Predict It.” Journal of School Choice: International Research and Reform Free Working Paper Version\nGoldhaber, D., Ronfeldt, M., Cowan, J., Gratz, T., Bardelli, E., & Truwit, M. (2022). “Room for improvement? Mentor teachers and the evolution of teacher preservice clinical evaluations.” American Educational Research Journal Free Open Access Paper\nGratz, T., Goldhaber, D., Willgerodt, M., & Brown, N. (2021). “The Front-Line Health Care Workers in Schools: Health Equity, the Distribution of School Nurses, and Student Access.” The Journal of School Nursing. Free Working Paper Version\nTheobald, R., Plasman, J., Gottfried, M., Gratz, T., Holden, K., & Goldhaber, D. (2021). “Sometimes less, sometimes more: Trends in career and technical education participation for students with disabilities.” Educational Researcher. Free Working Paper Version\nLong, M., Goldhaber, D., & Gratz, T. (2020). “Washington’s College Bound Scholarship Program and its Effect on College Entry, Persistence, and Completion.” Education Finance and Policy. Free Working Paper Version\nGoldhaber, D., Long, M., Gratz, T., & Rooklyn, J. (2019). “Pledging to Do ‘Good’: An Early Commitment Pledge Program, College Scholarships, and High School Outcomes in Washington State.” Educational Evaluation and Policy Analysis. Free Working Paper Version\nGoldhaber, D., Long, M. C., Person, A. E., Rooklyn, J., & Gratz, T. (2019). “Sign Me Up: The Factors Predicting Students’ Enrollment in an Early-Commitment Scholarship Program.” AERA Open. Free Open Access Paper\nTheobald, R., Goldhaber, D., Gratz, T., & Holden, K. (2018). “High school English Language Arts teachers and postsecondary outcomes for students with and without disabilities.” Journal of Disability Policy Studies. Free Working Paper Version\nTheobald, R., Goldhaber, D., Gratz, T., & Holden, K. (2018). “Career and Technical Education, Inclusion, and Postsecondary Outcomes for Students With Learning Disabilities.” Journal of learning disabilities. Free Working Paper Version\nGoldhaber, D., Gratz, T., & Theobald, R. (2017). “What’s in a teacher test? Assessing the relationship between teacher licensure test scores and student STEM achievement and course-taking.” Economics of Education Review. Free Working Paper Version\nPfeiffer, L., & Gratz, T. (2016). “The effects of rights-based fisheries management on risk taking and fishing safety.” Proceedings of the National Academy of Sciences. Free Open Access Paper"
  },
  {
    "objectID": "Publications.html#other-publications",
    "href": "Publications.html#other-publications",
    "title": "Publications",
    "section": "Other Publications",
    "text": "Other Publications\n\nDiagnosing Chagas Disease with Deep Neural Networks\nExamining HUD Housing Policy Through The Potential Outcomes Model"
  },
  {
    "objectID": "Prioritization.html",
    "href": "Prioritization.html",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "",
    "text": "Derived mathematical relationship between housing intervention prioritization and treatment effects for exiting homelessness\nUsed this mathematical relationship to identify a misalignment between the Department of Housing and Urban Development(HUD) policy objectives and optimal resource allocation\nDeveloped an optimization framework that balances maximizing exits from homelessness with equitable resource allocation\nCreated a dynamic weighting algorithm that ensures Fair Housing Act compliance\nAchieved resource allocation that maximizes exits from homelessness while maintaining equity ratios within ±5% of parity"
  },
  {
    "objectID": "Prioritization.html#tldr",
    "href": "Prioritization.html#tldr",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "",
    "text": "Derived mathematical relationship between housing intervention prioritization and treatment effects for exiting homelessness\nUsed this mathematical relationship to identify a misalignment between the Department of Housing and Urban Development(HUD) policy objectives and optimal resource allocation\nDeveloped an optimization framework that balances maximizing exits from homelessness with equitable resource allocation\nCreated a dynamic weighting algorithm that ensures Fair Housing Act compliance\nAchieved resource allocation that maximizes exits from homelessness while maintaining equity ratios within ±5% of parity"
  },
  {
    "objectID": "Prioritization.html#overview",
    "href": "Prioritization.html#overview",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Overview",
    "text": "Overview\nIn 2023, over half a million people in the United States experienced homelessness on a single night, with this population continuing to grow despite substantial government funding. Nevertheless, these resources remain insufficient to meet the need.\nContinuums of Care, the local agencies overseeing housing assistance, must prioritize households based on observable characteristics for receipt of scarce housing resources. HUD’s current recommendations direct prioritization scores to be based on barriers to obtaining housing and household vulnerabilities. When housing slots become available, households with the highest prioritization scores are selected for interventions such as rapid rehousing (RRH), transitional housing (TSH), or permanent supportive housing (PSH). Due to resource constraints, most eligible households do not receive housing interventions.\nThis project addresses a critical question in homelessness policy: How can we allocate scarce housing interventions to maximize successful exits from homelessness while ensuring an equitable distribution across protected demographic groups?\nBuilding on the work of Kube et al. (2023), this project developed and expanded an optimization framework that not only maximizes system efficiency but also incorporates equity constraints to prevent disparate impacts. This approach demonstrates how data science can support effective policymaking and balance competing policy objectives. A full write-up is available here"
  },
  {
    "objectID": "Prioritization.html#key-insights",
    "href": "Prioritization.html#key-insights",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Key Insights",
    "text": "Key Insights\nAnalysis of housing intervention data revealed three critical findings:\n\nPolicy misalignment: Current HUD prioritization guidance (CPD-17-01) emphasizes barriers to obtaining housing and vulnerability factors rather than their responsiveness to housing interventions, as measured by change in the likelihood of a successful exit in response to a housing intervention. Focusing on a household’s response to housing interventions aligns with the objective of exiting the greatest number of people from homelessness. Approaches that do not focus on a household’s responsiveness to interventions may reduce overall system effectiveness.\nIntervention effectiveness disparities: The effectiveness of interventions, i.e., conditional average treatment effects (CATEs), vary significantly across demographic groups. For example rapid rehousing shows a stronger positive impact on the likelihood of a successful exit from homelessness for female-identifying heads of households (see Figure 1). Simulated intervention effectiveness may be used as an exploratory data analysis tool for understanding potential disproportionate assignment to housing interventions.\nEquity-efficiency tradeoff: Without adjustment, allocating housing interventions based purely on maximizing exits from homelessness creates inequitable resource distributions. Risk-ratio penalization in the optimization cost function can balance the trade-offs inherent between the most efficient allocations, as measured by the most number of exiting households, and equitable allocations."
  },
  {
    "objectID": "Prioritization.html#results",
    "href": "Prioritization.html#results",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Results",
    "text": "Results\nOur report demonstrates that maximizing the number of households exiting homelessness requires strategic allocation of housing resources. Specifically, interventions should be directed toward households with the greatest increase in their likelihood of exiting homelessness when receiving support versus none.\nFigure 1 below depicts the distribution of the change in the likelihood of exiting homelessness (.i.e., CATEs) for rapid rehousing by whether the head of household identified as female or male. Figure 1 clearly shows that female-identifying households are more responsive to housing interventions.\nFigure 1: Rapid Re-Housing Treatment Effects by Gender\n\n\n\n\n\nBased on Figure 1, a naïve implementation of the optimization algorithm proposed by Kube et al. (2023) would prioritize female-identifying heads of households for rapid rehousing (RRH), as they show higher predicted treatment effects. While this would increase overall exits from homelessness, it would result in inequitable resource allocation.\nOur approach addresses this by leveraging the sequential nature of housing intervention availability. As slots open over time, we track cumulative allocations by subgroup and adjust the optimization objective using a dynamic penalty for over- or under-representation. This ensures more equitable distribution of resources across groups without sacrificing performance.\nA risk ratio is the proportion of a subgroup’s intervention assignments relative to their proportion in the overall population. A risk ratio of 1 indicates perfect parity, while values above or below 1 reflect over- or under-assignment, respectively.\nFigure 2 shows the cumulative allocation of housing interventions over time for male and female-identifying heads of households, alongside their corresponding risk ratios. The plot on the right illustrates the shift in risk ratios before and after applying equity weights.\nFigure 2: Equity-Weighted Optimization Results (C=0.49)\n\n\n\n\n\nBy applying a weighting factor, the model maintained risk ratios within ±5% of parity while still maximizing exits from homelessness. The final equity-weighted model achieved risk ratios of 0.96 for male-headed households and 1.04 for female-headed households, demonstrating both strong system performance and equitable allocation."
  },
  {
    "objectID": "Prioritization.html#methods",
    "href": "Prioritization.html#methods",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Methods",
    "text": "Methods\n\nOverview\nThis study develops a method for allocating scarce housing interventions in a way that maximizes the number of households exiting homelessness. The method is based on the optimization framework proposed by Kube et al. (2023) and is extended to address equity concerns under the Fair Housing Act.\nThe methodology consists of three steps:\n\nEstimate exit probabilities for each household with and without intervention.\nOptimize intervention assignments to maximize expected exits from homelessness.\nIntroduce equity constraints to ensure fair allocation across protected subgroups.\n\n\n\n\nPredicting Exit from Homelessness\nWe trained predictive models to estimate the likelihood that a household exits homelessness under different treatment conditions. The models predict the binary outcome of a successful exit (i.e., stable housing with no reentry within six months).\nSeveral configurations were tested:\n\nAlgorithms: Random Forest, Elastic Net\nFeature sets: head-of-household variables, aggregated household variables, and both\nEncodings: one-hot encoding and weight-of-evidence\nTransformations: standardization, log transformation of skewed variables\nDimensionality reduction: with and without PCA (retaining 95% variance)\n\nA Random Forest classifier performed best, with an AUROC of 0.84 on validation data.\n\n\n\nOptimization to Maximize Exits\nLet:\n\n\\(p_{ijt}\\) be the predicted probability that household \\(i\\) exits homelessness in week \\(t\\) if assigned intervention \\(j\\). No intervention is included in the set of “interventions” J, and corresponds to \\(j=0\\)\n\\(x_{ijt}\\) be a binary indicator of whether household \\(i\\) is assigned intervention \\(j\\) in week \\(t\\)\n\\(C_{jt}\\) be the number of available slots for intervention \\(j\\) in week \\(t\\)\n\nThe objective is to maximize the expected number of exits:\n\\[\n\\max_{x_{ijt}} \\sum_{i=1}^{N} \\sum_{j} p_{ijt} \\cdot x_{ijt}\n\\]\nSubject to:\n\nCapacity constraints: \\(\\sum_i x_{ijt} = C_{jt}\\) for all \\(j \\ne 0\\)\nOne intervention per household: \\(\\sum_j x_{ijt} = 1\\)\n\\(x_{ijt} \\in \\{0, 1\\}\\)\n\nThis integer program is solved weekly as new capacity becomes available.\n\n\n\nRole of Conditional Average Treatment Effects (CATE)\nIn a simplified setting (one intervention, two households, one slot), the optimization reduces to selecting the household with the largest difference in predicted outcomes with vs. without treatment:\n\\[\n\\max \\left[ x_{1,\\text{tx}}(p_{1,\\text{tx}} - p_{1,\\text{ntx}}) + x_{2,\\text{tx}}(p_{2,\\text{tx}} - p_{2,\\text{ntx}}) \\right]\n\\]\nSubject to:\n\\[\nx_{1,\\text{tx}} + x_{2,\\text{tx}} = 1\n\\]\nThus, we assign the intervention to the household with the largest conditional average treatment effect.\n\n\n\nIncorporating Equity\nTo avoid disproportionately assigning interventions to certain subgroups, we track assignment rates over time and modify the optimization.\nLet:\n\n\\(\\alpha_g\\) be the historical proportion of group \\(g\\)\n\\(\\gamma_{gt^*}\\) be the cumulative assignment rate to group \\(g\\) up to time \\(t^*\\)\n\\(r_{gt^*} = \\gamma_{gt^*} / \\alpha_g\\) be the risk ratio\n\nWe introduce an equity penalty weighted by a tunable hyperparameter \\(C\\):\n\\[\n\\max_{x_{ijt}} \\sum_{i,j,t} p_{ijt} \\cdot x_{ijt} + \\sum_{j \\ne 1} C \\cdot (1 - r_{gt^*}) \\cdot x_{ijt}\n\\]\n\nWhen a group is under-assigned, \\(r_{gt^*} &lt; 1\\), and the penalty is positive — boosting that group’s priority.\nWhen over-assigned, the penalty is negative.\n\nWe select \\(C\\) by grid search to ensure long-run risk ratios remain within ±5% of parity across groups.\n\n\n\nPractical Considerations\n\nA short “burn-in” period may be needed to stabilize risk ratios before applying equity weights.\nAlternatively, historical assignment data can be used to initialize risk ratios.\nAdministrators can monitor and adjust the weights dynamically as population needs and effectiveness change."
  },
  {
    "objectID": "Prioritization.html#technical-challenges-and-solutions",
    "href": "Prioritization.html#technical-challenges-and-solutions",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Technical Challenges and Solutions",
    "text": "Technical Challenges and Solutions\nSeveral technical challenges were addressed during this project:\n\nData quality issues: Like many social service datasets, the homelessness data contained substantial missing values, particularly for exit destinations (69% unknown for non-intervention cases). This required careful handling to prevent biased estimates.\nCausal inference: The observational nature of the data required careful consideration of selection bias. The programmatic design of housing intervention assignment provided a pseudo-randomization mechanism conditional on observable characteristics.\nDynamic optimization: The algorithm needed to respond to changing population compositions over time, requiring a burn-in period to establish stable risk ratios."
  },
  {
    "objectID": "Prioritization.html#policy-implications",
    "href": "Prioritization.html#policy-implications",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Policy Implications",
    "text": "Policy Implications\nThis work has several important implications for homelessness policy:\n\nCurrent HUD prioritization guidance may need revision to better align with the goal of maximizing successful exits from homelessness\nMachine learning approaches can significantly improve resource allocation efficiency, but must be carefully designed to avoid perpetuating inequities\nContinuous monitoring and adjustment of allocation algorithms is necessary to maintain equity as population compositions change"
  },
  {
    "objectID": "Prioritization.html#acknowledgments",
    "href": "Prioritization.html#acknowledgments",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis analysis builds upon the work of Kube et al. (2023) on efficient allocation of homelessness resources. All data was provided by a large Continuum of Care through their Homelessness Management Information System (HMIS). Trevor Gratz ideated, scoped, developed the connection between optimizations and CATEs, created the penalized optimization algorithm, and wrote the associated paper. Anu Zan implemented the optimization algorithm and created Figure 2."
  },
  {
    "objectID": "Prioritization.html#references",
    "href": "Prioritization.html#references",
    "title": "Optimizing Housing Interventions for Equitable Homelessness Reduction",
    "section": "References",
    "text": "References\n\nKube, A. R., Das, S., & Fowler, P. J. (2023). Fair and efficient allocation of scarce resources based on predicted outcomes: implications for homeless service delivery. Journal of Artificial Intelligence Research, 76, 1219-1245.\nAuditor of the State of California (2024, April). Homelessness in California. Report 2023-102.1\nChelmis, C., Qi, W., & Lee, W. (2021, April). Challenges and opportunities in using data science for homelessness service provision. In Companion proceedings of the web conference 2021 (pp. 128-135).\nMeyer, B. D., Wyse, A., & Logani, I. (2023). Life and death at the margins of society: the mortality of the US homeless population. National Bureau of Economic Research."
  },
  {
    "objectID": "Chagas.html",
    "href": "Chagas.html",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "",
    "text": "Built and trained a deep neural network (1D CNN) to detect Chagas disease from ECG signals.\nAchieved an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.84 on unseen data.\nDeployed an interactive app to test predictions on user-uploaded ECGs.\nApplied best practices in deep learning, model evaluation, and deployment."
  },
  {
    "objectID": "Chagas.html#tldr",
    "href": "Chagas.html#tldr",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "",
    "text": "Built and trained a deep neural network (1D CNN) to detect Chagas disease from ECG signals.\nAchieved an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.84 on unseen data.\nDeployed an interactive app to test predictions on user-uploaded ECGs.\nApplied best practices in deep learning, model evaluation, and deployment."
  },
  {
    "objectID": "Chagas.html#overview",
    "href": "Chagas.html#overview",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "Overview",
    "text": "Overview\nChagas disease is a parasitic illness transmitted by insects, affecting an estimated 8 million people worldwide and causing over 4,750 deaths annually. While most individuals show no noticeable symptoms at the time of initial infection, the disease can progress silently over time, ultimately leading to heart failure in chronic cases.\nThis project builds on the work of Jidling et al. (2023), who developed a convolutional neural network (CNN) to detect Chagas disease using electrocardiogram (ECG) data. Expanding on their approach, I incorporate the sequential nature of ECG signals by designing a hybrid deep learning model that combines convolutional and transformer architectures. This enhancement significantly improves diagnostic performance: on the validation set, the area under the receiver operating characteristic curve (AUC-ROC) increases from 0.80 with the CNN alone to 0.84 with the combined model. Improving early detection of undiagnosed cases could lead to life-saving interventions for hundreds of patients.\nBelow, you’ll find an embedded version of the hybrid model. It is followed by a detailed schematic of the architecture and a write-up of this project, which was inspired by the PhysioNet 2025 challenge."
  },
  {
    "objectID": "Chagas.html#try-it-yourself",
    "href": "Chagas.html#try-it-yourself",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "🔍 Try It Yourself",
    "text": "🔍 Try It Yourself\nUpload your own ECG input to see predictions from the trained deep learning model. Don’t have ECG data on hand? Try this sample from CODE-15 and SaMi-Trop: sample."
  },
  {
    "objectID": "Chagas.html#results",
    "href": "Chagas.html#results",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "Results",
    "text": "Results\nFigure 1 below plots the loss curves for the training and validation set by epoch. The validation loss and AUROC scores plateau by the 6th epoch. Nevertheless, the validation AUROC achieved state-of-the-art results for diagnosing Chagas disease. The best AUROC validation score was 0.84, which constitutes a significant improvement over the 0.80 AUROC score in the convolution only model explored in Jidling et al. (2023).\nFigure 1: Loss and AUROC curves demonstrate stable training and generalization over 10 epochs.\n\n\n\n\n\nThe training metrics suggest good model convergence without signs of overfitting. To assess prediction quality at the instance level, we now examine the confusion matrix. Binary predictions were made using a 0.5 probability threshold.\nTable 1: Hybrid Convolution Plus Transformer Validation Confusion Matrix\n\n\n\nActual \\ Predicted\nPositive\nNegative\n\n\n\n\nPositive\n1151\n423\n\n\nNegative\n294\n1069\n\n\n\nThe model achieves a recall of 0.72 and a precision of 0.78. In clinical contexts, a false negative (i.e., informing a patient they are Chagas-free when they are not) is typically considered a worse error to make. The threshold on this model could be lowered to lower the incidence of false negatives. The implications of this would be to increase false positives, lower precision, but to increase recall."
  },
  {
    "objectID": "Chagas.html#methods",
    "href": "Chagas.html#methods",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "Methods",
    "text": "Methods\nThe model is a 1D CNN optimized for time-series ECG data. It extracts temporal patterns in raw signals to distinguish Chagas-positive and negative cases with high performance.\nFigure 2 below displays the architecture used to diagnose Chagas disease. For a full discussion of the experiments and their results, please read the following paper.\nFigure 2: 1D Convolutional Neural Network architecture for ECG signal classification.\n\n\n\n1D CNN Architecture"
  },
  {
    "objectID": "Chagas.html#acknowledgments",
    "href": "Chagas.html#acknowledgments",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe following model was developed using CODE-15 and SaMi-Trop data as part of the George B. Moody PhysioNet Challenge 2025.\nThis application was inspired by a collaborative project originally developed by a team of three: myself, Marc Lafargue, and Matheus Rama Amorim, as part of our coursework at the Georgia Institute of Technology."
  },
  {
    "objectID": "Chagas.html#references",
    "href": "Chagas.html#references",
    "title": "Diagnosing Chagas disease with deep neural networks",
    "section": "References",
    "text": "References\n\nCarl Jidling, Daniel Gedon, Thomas B Sch¨on, Claudia Di Lorenzo Oliveira, Clareci Silva Cardoso, Ariela Mota Ferreira, Luana Giatti, Sandhi Maria Barreto, Ester C Sabino, Antonio LP Ribeiro, et al. Screening for chagas disease from the electrocardiogram using a deep neural network. PLoS Neglected Tropical Diseases, 17(7):e0011118, 2023.\nFrancisco Rogerlˆandio Martins-Melo, Marcia C Castro, and Guilherme Loureiro Werneck. Levels and trends in chagas disease-related mortality in brazil, 2000–2019. Acta Tropica, 220:105948, 2021.\nZulma M Cucunub´a, Sebasti´an A Guti´errez-Romero, Juan-David Ram´ırez, Natalia Vel´asquez-Ortiz, Soledad Ceccarelli, Gabriel Parra-Henao, Andr´es F Henao-Mart´ınez, Jorge Rabinovich, Mar´ıa-Gloria Bas´a˜nez, Pierre Nouvellet, et al. The epidemiology of chagas disease in the americas. The Lancet Regional Health–Americas, 37, 2024."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Trevor Gratz",
    "section": "About Me",
    "text": "About Me\nI have a background in mathematics, statistics, and data science, and I’ve worked in academia and the public sector. I received my Bachelor of Science in Mathematics and Statistics in 2014 from the University of Washington, and my Master of Science in Data Analytics from Georgia Tech in August, 2025. I have over a decade of work experience in data analytics and am a co-author on over ten peer reviewed publications cited by over 500 journal articles. My work emphasizes:\n\nLanguages: Python, R, SQL, Stata\n\nLibraries/Frameworks: scikit-learn, pandas, NumPy, PyTorch, Geopandas, Statsmodels\n\nTools: git, Jupyter, VS Code\n\nTechniques: Machine Learning, Causal Inference, Optimization, Regression, Classification"
  },
  {
    "objectID": "index.html#skills-summary",
    "href": "index.html#skills-summary",
    "title": "Trevor Gratz",
    "section": "Skills Summary",
    "text": "Skills Summary\n\nLanguages: Python, R, SQL\nTools & Libraries: scikit-learn, pandas, NumPy, Pytorch, Matplotlib, Geopandas, Statsmodels, git\nTechniques: Machine Learning, Optimization, Causal Inference, Regression, Classification"
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Trevor Gratz",
    "section": "Featured Projects",
    "text": "Featured Projects\nHere are a few highlighted projects that demonstrate my capabilities:\n\nDiagnosing Chagas Diseases with Deep Convolutional Neural Networks\nCreated a hybrid convolutional neural network plus transformer model to diagnose Chagas disease from electrocardiogram data.\n👉 View Project ›\n\n\n\nPrioritization and equitable allocation of housing interventions for homeless households\nBuilt upon existing literature to develop a weighted optimization algorithm that balances exiting the greatest number of households from homelessness while minimizing inequitable housing resource allocation.\n👉 View Project ›\n\n\n\nMobile medication assisted therapy for substance use disorder treatment site location\nWorked with program managers to create an interactive application allowing for the exploration of site locations based on historical overdose calls data.\n👉 View Project ›"
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Trevor Gratz",
    "section": "Get in Touch",
    "text": "Get in Touch\nI’m always open to opportunities for collaboration, research, or employment. You can reach me at:\n\n📧 trevormgratz@gmail.com\n💼 LinkedIn\n🐙 GitHub"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Diagnosing Chagas Diseases with Deep Convolutional Neural Networks\nCreated a hybrid convolutional neural network plus transformer model to diagnoses Chagas disease from electrocardiogram data. 👉 View Project ›\n\n\n\nPrioritization and equitibale allocation of housing interventions for homeless houeholds\nBuilt upon existing literature to develop a weighted optimization algorithm that balances exiting the greatest number of households from homelessness while minimizing inequitable houseing resource allocation.\n👉 View Project ›\n\n\n\nMobile medication assisted therapy for substance use disorder treatment site location\nWorked with program managers to create an interactive application allowing for the exploration site locations based on historical overdose calls data.\n👉 View Project ›"
  },
  {
    "objectID": "RCT.html",
    "href": "RCT.html",
    "title": "Using machine learning and optimization for resource planning: a case study in locating opioid use disorder treatment units",
    "section": "",
    "text": "Opioid use disorder has been widely viewed as a growing public health emergency. Medication assisted therapy (MAT) is considered the gold standard for treating opioid use-disorder: MAT can reduce overdose mortalty by up to three fold. However, facilities that serve residents are often in urban settings, and MAT can be difficult to access outside of major metropolitan areas. Pierce County is addressing the needs of rural residence by provisioning mobile opioid use disorder treatment vans.\nThe analyses and tools presented here support the planning phase of rolling out mobile opioid-use-disorder van. It also supported the design process of a randomized controlled trial(RCT) of the effectiveness of mobile opioid-use-disorder vans on reducing substance abuse. Pierce County partnered with UC Santa Cruz to plan the RCT.\nThis application plots potential site locations for mobile opioid-use-disorder treatment vans, their location relative to the number of overdose 911 related calls, and their location relative to main road and transit. Potential sites have been clustered into high, medium, and low needs areas based on overdose call locations and accessibility.\n\n\n\n\n\nCluster analysis revealed three kinds of potential site: high call volume and high accessibility, medium call volume and medium accessibility, and low call volume and low accessibility. The site characteristics by cluster are depicted in Table 1 below.\n\n\nTable 1. Site Mean and Standard Deviation by Cluster\n\n\n\n\n\n\n\n\n\nVariable\nCluster 1 (Low)\nCluster 2 (Medium)\nCluster 3 (High)\n\n\n\n\nCalls within 500 meters\n0.2 (0.4)\n1.1 (1.3)\n3.4 (1.1)\n\n\nCalls within 1,000 meters\n0.6 (1.0)\n3.9 (2.9)\n17 (11.2)\n\n\nCalls within 2,000 meters\n1.6 (2.8)\n9.5 (5.9)\n40.4 (9.3)\n\n\nCalls within 3,000 meters\n3.0 (4.6)\n17.1 (14.5)\n61.6 (14.0)\n\n\nMeters to transit\n16,389 (17,925)\n1,258 (1,975)\n239 (224)\n\n\nMeters to closest major road\n1,269 (1,747)\n481 (626)\n184 (100)\n\n\n\n\n\nOutside of Tacoma, overdose calls and accessibility were highest along the highway 7 and WA route 161 corridors. This is evidenced from the calls data themselves, but also the categorization of potential sites along these corridors into cluster 3.\n\n\n\n\n\n2023 South Sound 911 Overdose Calls\nPierce County data on:\n\nFire stations\nLibraries\nSafe Parking sites\nRoads\nBus stops\n\nFixed MAT Locations\n\n\n\n\n\n\nSubject matter experts at Pierce County Human Services identified fire stations, libraries, and safe parking sites as potential partners for locating a mobile opioid use-disoreder van on the premesis. For each potential site i, I calculated the number of calls within 500, 1000, 2000, and 3000 meters. Because we want to prioritize potential sites with ease of access, I also calculate the distance to the closest bus stop and major road. I omitted sites in the City of Tacoma due to it’s high presence of fixed MAT location and greater access to services.\nAfter applying a standard scalar feature normalization I performed K-means clustering on the feature set. K-means applies equal weighting to all features. However, from the context it is clear that the proximity of overdose calls to potential sites is more relevant than access to the site alone. The multiple features for the number of calls within different specified radii of the site increase the relative importance of the number of calls nearby, as well as capture information from different ranges of distance to sites.\nThe site information, datasets, and clustering algorithm were packaged and ported into the Streamlit applicaiton. This allows the user to explore potential sites other than the pre-calculated sites at fire statations, libraries and safe parking.\nThis application alone was sufficient to help Pierce County Human Service’s program managers think through the implications of different site locations. However, it is unable to help researchers think through the implications of site selection on statistical power.\n\n\n\nThis methodology outlines a systematic approach to selecting experimental sites for a block-randomized trial in order to maximize statistical power while minimizing the risk of treatment spillover between sites. The statistical framework is based on the model of one treated unit per block with potentially multiple control units, following the approach of Pashley and Miratrix (2021). Power is calculated using pre-treatment outcome data to estimate within-block variances, and is directly linked to minimizing the variance of the treatment effect estimator. Key assumptions include homoskedasticity within blocks, independence between treatment and control units, and block-level heterogeneity in variance.\nTo operationalize site selection, a three-stage approach is developed. First, the method uses integer programming to select the largest possible number of sites that satisfy a minimum distance constraint, preventing spillover. Second, k-means clustering is applied to group selected sites into blocks with similar pre-treatment outcome values, minimizing within-block variance. Finally, statistical power is calculated based on this block structure using standard power formulas. This approach balances competing design goals: maximizing sample size, ensuring spatial independence, and minimizing outcome variance, ultimately yielding a more efficient and robust trial design.\n\n\n\n\n\n\nTechnical Details: Statistical Framework and Implementation\n\n\n\n\n\n\n\nThis document outlines the methodology used to select sites so as to maximize statistical power. This report begins by discussing the statistical model used in a block-randomized trial with exactly one treated unit per block and potentially multiple control units, where block sizes may be unbalanced. The approach leverages pre-treatment data to estimate variance components and follows the framework established by Pashley and Miratrix (2021) for calculating power. It discusses the potential of spillover effects in a spatial randomization, and develops a methodology to maximize statistical power while minimizing the potential for spillover.\n\n\n\nThe basic model for a block-randomized trial can be expressed as:\n\\[Y_{ik} = \\alpha_k + \\tau Z_{ik} + \\varepsilon_{ik}\\]\nWhere: - \\(Y_{ik}\\) is the outcome for unit \\(i\\) in block \\(k\\) - \\(\\alpha_k\\) is the block-specific fixed effect - \\(\\tau\\) is the treatment effect (our parameter of interest) - \\(Z_{ik}\\) is the treatment indicator (1 for treatment, 0 for control) - \\(\\varepsilon_{ik}\\) is the random error term\n\n\n\nFollowing Pashley and Miratrix (2021), the blocked estimator of the average treatment effect is:\n\\[\\hat{\\tau}_{(BK)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\hat{\\tau}_k\\]\nWhere: - \\(K\\) is the total number of blocks - \\(n_k\\) is the number of units in block \\(k\\) (including both treated and control) - \\(n = \\sum_{k=1}^{K} n_k\\) is the total sample size - \\(\\hat{\\tau}_k\\) is the estimated treatment effect within block \\(k\\)\nFor our design with exactly one treated unit per block, \\(\\hat{\\tau}_k\\) is:\n\\[\\hat{\\tau}_k = Y_k^T - \\bar{Y}_k^C\\]\nWhere \\(Y_k^T\\) is the outcome for the treated unit in block \\(k\\) and \\(\\bar{Y}_k^C\\) is the mean outcome for the control units in block \\(k\\).\n\n\n\nPashley and Miratrix (2021) provide the variance formula for the blocked estimator in their Equation (2). Adapting this for our specific case with one treated unit per block and assuming independence between treatment and control units within blocks:\n\\[Var(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\left(\\frac{n_k}{n}\\right)^2 Var(\\hat{\\tau}_k)\\]\nWithin each block, the variance of the block-specific treatment effect is:\n\\[Var(\\hat{\\tau}_k) = Var(Y_k^T) + Var(\\bar{Y}_k^C) = \\sigma_k^2 + \\frac{\\sigma_k^2}{n_k^C}\\]\nWhere: - \\(\\sigma_k^2\\) is the variance of outcomes within block \\(k\\) - \\(n_k^C = n_k - 1\\) is the number of control units in block \\(k\\)\nSimplifying:\n\\[Var(\\hat{\\tau}_k) = \\sigma_k^2\\left(1 + \\frac{1}{n_k^C}\\right) = \\sigma_k^2\\left(\\frac{n_k^C + 1}{n_k^C}\\right) = \\sigma_k^2\\left(\\frac{n_k}{n_k - 1}\\right)\\]\nTherefore, the variance of our treatment effect estimator is:\n\\[Var(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\left(\\frac{n_k}{n}\\right)^2 \\sigma_k^2\\left(\\frac{n_k}{n_k - 1}\\right) = \\sum_{k=1}^{K} \\frac{n_k^3}{n^2(n_k - 1)}\\sigma_k^2\\]\nFor this analysis, we make the following assumptions:\n\nHomoskedasticity within blocks: The variance of outcomes is constant within each block (i.e., treated and control units have the same variance within a block). This assumption is necessary as we have no pre-treatment data on treated outcomes with which to estimate the variance.\nIndependence: Treatment and control outcomes are independent within blocks, conditional on block membership.\nBlock-specific heterogeneity: The variance can differ across blocks (i.e., \\(\\sigma_k^2\\) can vary with \\(k\\)).\nTreatment affects means only: The treatment affects the mean of the outcome but not its variance.\nPre-treatment data is sufficient to estimate outcome variance post-treatment: The estimation of statistical power, as shown in in section 6, relies on an accurate estimation of the variance of our treatment effect.\n\n\n\n\nWe can leverage pre-treatment outcome data to estimate the variance components required for power analysis:\n\\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1} \\sum_{i=1}^{n_k} (Y_{ik,pre} - \\bar{Y}_{k,pre})^2\\]\nWhere: - \\(Y_{ik,pre}\\) is the pre-treatment outcome for unit \\(i\\) in block \\(k\\) - \\(\\bar{Y}_{k,pre}\\) is the mean pre-treatment outcome in block \\(k\\)\nThese estimates can then be substituted into the variance formula:\n\\[\\widehat{Var}(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\frac{n_k^3}{n^2(n_k - 1)}\\hat{\\sigma}_k^2\\]\n\n\n\nFor a given significance level \\(\\alpha\\) (typically 0.05) and a minimum detectable effect size \\(\\delta\\), the power of the test is:\n\\[Power = 1 - \\beta = \\Phi\\left(\\frac{|\\delta|}{\\sqrt{\\widehat{Var}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2}\\right)\\]\nWhere: - \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution - \\(z_{1-\\alpha/2}\\) is the critical value (e.g., 1.96 for \\(\\alpha = 0.05\\) with a two-sided test)\nAlternatively, to determine the required sample size for a desired power level \\(1-\\beta\\), we need to solve:\n\\[\\frac{|\\delta|}{\\sqrt{\\widehat{Var}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2} = z_{1-\\beta}\\]\nWhich gives:\n\\[\\widehat{Var}(\\hat{\\tau}_{(BK)}) = \\frac{\\delta^2}{(z_{1-\\alpha/2} + z_{1-\\beta})^2}\\]\nWe can then determine how many blocks or how many units per block would be needed to achieve this target variance.\n\n\n\nOur goal is to select sites to maximize statisticl power, while ensuring there is no spill over of treatment effects between sites. We can minimize the potential of spill over effects by ensuring all selected sites are at a minimum of X meters away from each other. The optimization progam is formed as follows:\nLet: - \\(S_{i,k}\\) be a 0/1 indicator for whether site \\(i\\) is in block \\(k\\) - \\((L_{i,1}, L_{i, 2})\\) be the coordinates of site \\(i\\) in a projection (EPSG:6559) such that the euclidian distance between two coordinates is meters - \\(D_{min}\\) be the minimum distance two sites must be apart for spill over to be neglegble\nThe objective function is:\n\\[ argmax_{S_{i,g}} \\Phi\\left(\\frac{|\\delta|}{\\sqrt{\\widehat{Var}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2}\\right)\\]\nSubject to:\n\\[ \\sum_{g} S_{i,g} \\leq 1 \\quad \\forall i \\]\n\\[\\sqrt{(L_{i,1}-L_{j,1})^2 + (L_{i,2}-L_{j,2})^2} \\geq (\\sum_{g} S_{i,g})(\\sum_{g} S_{j,g})D_{min} \\quad \\forall i, j \\quad if \\: i\\neq j   \\]\nHere the first constraint ensures each site can belong to only one block at most. The second constraint requires selected sites only to be at least \\(D_{min}\\) meters away from each other.\nThis optimiztion problem is constrained, highly non-linear, and is a mixed binary and continous problem. It not tractable with existing methods. However, examining the factors that influence the power calculation can yield insight into how to design an approach that may approximate site selection for maximal power. Specifically, power be maximized when the variance of the treatment effect estimator is minimized. Examining the formula for the treatment effect varience shows that it is minimized when sample sizes increase and when the within-block variances are minimized. Unfortunately, increasing sample sizes and minimizing within-block variance may not always be compatible. In section 9 below we detail an approach that weighs tradeoffs between maximizing sample size and minimizing within-block variance while ensure negligille spill over.\n\n\n\nOur approach to maximizing power while preventing spillover effects combines three stages:\n\n\nIn the first stage, we select the maximum number of sites that satisfy the distance constraint to prevent spillover effects while also filter to sites within pre-specified ranges of the outcome variable of interest. For example, we filter to all sites with more than 10 calls within 3,000 meters in a year. This filtering is aimed at reducing within-block variance prior to site selection, as the site selection process itself is ignorant of within-block variance. After prefiltering, we solve for the maximum number of sites.\nThis optimization is formulated as an integer programming problem:\nLet \\(S\\) be the set of all potential sites, and for each site \\(i \\in S\\): - \\(S_i\\) is a binary indicator variable where \\(S_i = 1\\) if site \\(i\\) is selected, and \\(S_i = 0\\) otherwise - \\(D_{i,j}\\) is a binary indicator where \\(D_{i,j} = 1\\) if the distance between sites \\(i\\) and \\(j\\) is greater than the minimum required distance (or if \\(i = j\\)), and \\(D_{i,j} = 0\\) otherwise. This reformulation removes the square-root constraint seen in section 7 and recasts the problem as a linear program.\nThe optimization problem is:\n\\[\\text{maximize } \\sum_{i \\in S} S_i\\]\nSubject to: \\[S_i + S_j \\leq D_{i,j} + 1 \\quad \\forall i,j \\in S\\]\nThis constraint ensures that if two sites \\(i\\) and \\(j\\) are both selected (\\(Z_i = Z_j = 1\\)), then they must be sufficiently far apart (\\(D_{i,j} = 1\\)). The constarint cannot be satisfied if both are selected and are too close.\n\n\n\nOnce we have selected the maximum number of eligible sites, we assign them to blocks to minimize within-block variance. For a design with exactly one treated unit per block, power is maximized when the within-block variance is minimized.\nWe use k-means clustering on the outcome variable to achieve this, where \\(k\\) equals the number of desired blocks:\n\nInitialize \\(k\\) centroids randomly from the outcome values of selected sites\nAssign each site to the nearest centroid based on the absolute difference in outcome values\nRecalculate centroids as the mean of outcome values in each cluster\nRepeat steps 2-3 until convergence\n\nFor one-dimensional clustering (i.e., clustering on a single outcome variable), the k-means loss function is:\n\\[\\sum_{k=1}^{K} \\sum_{i \\in C_k} (Y_i - \\mu_k)^2\\]\nWhere: - \\(C_k\\) is the set of sites assigned to block \\(k\\) - \\(Y_i\\) is the outcome value for site \\(i\\) - \\(\\mu_k\\) is the mean outcome value for block \\(k\\)\nThis loss function is directly related to minimizing the within-block variance, as the within-block variance for block \\(k\\) is:\n\\[\\sigma_k^2 = \\frac{1}{n_k-1} \\sum_{i \\in C_k} (Y_i - \\mu_k)^2\\]\nTherefore, minimizing the k-means loss function effectively minimizes the within-block variance, which maximizes power according to our variance formula in Section 4.\nAfter k-means clustering, we ensure each block has at least two sites (one treated, one control) by reassigning sites from larger blocks to smaller ones if necessary, based on outcome value similarity.\n\n\n\nWith the blocks established, we can calculate the power of our design using the formula from Section 6:\n\\[\\text{Power} = 1 - \\beta = \\Phi\\left(\\frac{|\\delta|}{\\sqrt{\\widehat{\\text{Var}}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2}\\right)\\]\nWhere the variance of the treatment effect estimator is:\n\\[\\widehat{\\text{Var}}(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\frac{n_k^3}{n^2(n_k - 1)}\\hat{\\sigma}_k^2\\]\nAnd \\(\\hat{\\sigma}_k^2\\) is estimated from the pre-treatment outcome data:\n\\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1} \\sum_{i=1}^{n_k} (Y_{ik} - \\bar{Y}_{k})^2\\]\nThe treatment effect size \\(\\delta\\) can be specified either as a fixed value or as a percentage of the average outcome across selected sites.\n\n\n\nKey considerations for implementation include:\n\nFiltering criteria: We apply filtering criteria to sites before stage 1 to ensure all candidate sites meet minimum requirements.\nMinimum selected sites: We require at least twice the number of blocks to be selected (to have at least one control unit per block). If fewer sites are selected, an error is raised.\nBlock balance: We enforce a minimum of two sites per block, which ensures we have at least one control unit per block.\nDistance calculation: Pairwise distances between sites are calculated using their geographic coordinates, typically in a projected coordinate system where Euclidean distance corresponds to meters.\nSignificance level: A standard significance level of α = 0.05 is typically used, corresponding to a critical value of \\(z_{1-\\alpha/2} = 1.96\\) for a two-sided test.\n\nBy following this three-stage approach, we maximize power by selecting the maximum number of sites subject to distance constraints and optimally assigning them to blocks to minimize within-block variance, while maintaining the ability to detect a specified treatment effect size with high probability.\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nRole in Power Formula\nRisk if Violated\n\n\n\n\nNormality of the estimator\nJustifies use of the standard normal distribution (\\(\\Phi\\)) for computing power.\nPower estimates may be inaccurate, especially if the number of blocks is small or outcome distributions are skewed.\n\n\nAccurate variance estimation\nRequired to plug into the denominator of the standardized effect size (\\(\\delta / \\sqrt{Var}\\)).\nUnder- or overestimation of variance leads to over- or underestimation of power.\n\n\nIndependence within blocks\nEnsures that variance components add properly; needed for block-specific variance formulas.\nCorrelated outcomes (e.g., due to interference or spillovers) can underestimate variance and inflate power.\n\n\nHomoskedasticity within blocks\nAssumes treated and control units within the same block have the same outcome variance.\nIf treatment changes variability, the variance formula for \\(\\hat{\\tau}_k\\) is invalid.\n\n\nConstant treatment effect\nAssumes a fixed treatment effect \\(\\delta\\) for the power calculation.\nIf effects vary across units/blocks, power calculations may not reflect the true ability to detect effects.\n\n\nCorrect \\(\\alpha\\) and \\(\\beta\\)\nDetermines critical values (\\(z_{1-\\alpha/2}\\), \\(z_{1-\\beta}\\)) used in the power formula.\nWrong values lead to incorrect inference thresholds (e.g., too lenient or too strict).\n\n\nNo covariate adjustment\nMatches the unadjusted variance formula derived from the block-only design.\nIf covariates are used in analysis but not in the power calculation, power may be underestimated.\n\n\n\n\n\n\nFor the results presented here, the parameters were:\n\nOutcome variable: Overdose calls within 3,000 meters\n\\(D_{min}\\): 6,000 meters\nFilter variable: Overdose calls within 3,000 meters\nFilter Threshold: Iterated over a minimum threshold starting at 0 and ending at 20 calls\nEffect size: 15% of the selected site mean\nNumber of blocks: 4\n\n\n\n\nFigure 1 below depicts the power calculations for selected sites based on the threshold minimum.\n\n\nWhile there is arrangement of sites with a power level greater than 80%, the analyses here represent a upper bound on the power calculations. This analysis is an upper bound on the power calculation because a number of key assumptions are likely violated. For instance, with only 4 blocks the normality of the estimator is unlikely to hold. Furthermore, the scenario where the power reached more than 80% had only 10 total sites selected. This means at most 3 sites per block were used to inform the estimate of the within-block variance.\nFigure 2 below plots the number of sites selected on the x-axis, the average within-block variance on the y-axsis, and a gradient and marker size value for the power with larger and redder markers indicating greater power.\n\n\nWhat is clear from figure 2 is that the arrangements with the highest power are driven almost entirely by reductions in the within-block variance. This combined with the low-sample sizes is concerning, since it could indicate our algorithm is overfitting. The low block sample sizes means our process for selecting blocks after site selection is likely underestimating the true within-block variance.\nThat being said, the sites that are selected do possess several desirable qualities. They are geographically distant from one another (see figure 3), have higher call volumnes than the average site, and have low-within block variance.\n\n\n\n\n\n\n\n\n\n\nContainerize with Docker and deploy via AWS\nConsider alternative RCT design. If applicable, create synthetic data and run power calculations.\n\n\n\n\nNational Academies of Sciences, Engineering, and Medicine. (2018). Medication-Assisted Treatment for Opioid Use Disorder.\nPashley, N. E., & Miratrix, L. W. (2021). Insights on Variance Estimation for Blocked and Matched Pairs Designs. Journal of Educational and Behavioral Statistics, 46(3), 271-296."
  },
  {
    "objectID": "RCT.html#key-findings",
    "href": "RCT.html#key-findings",
    "title": "Using machine learning and optimization for resource planning: a case study in locating opioid use disorder treatment units",
    "section": "",
    "text": "Cluster analysis revealed three kinds of potential site: high call volume and high accessibility, medium call volume and medium accessibility, and low call volume and low accessibility. The site characteristics by cluster are depicted in Table 1 below.\n\n\nTable 1. Site Mean and Standard Deviation by Cluster\n\n\n\n\n\n\n\n\n\nVariable\nCluster 1 (Low)\nCluster 2 (Medium)\nCluster 3 (High)\n\n\n\n\nCalls within 500 meters\n0.2 (0.4)\n1.1 (1.3)\n3.4 (1.1)\n\n\nCalls within 1,000 meters\n0.6 (1.0)\n3.9 (2.9)\n17 (11.2)\n\n\nCalls within 2,000 meters\n1.6 (2.8)\n9.5 (5.9)\n40.4 (9.3)\n\n\nCalls within 3,000 meters\n3.0 (4.6)\n17.1 (14.5)\n61.6 (14.0)\n\n\nMeters to transit\n16,389 (17,925)\n1,258 (1,975)\n239 (224)\n\n\nMeters to closest major road\n1,269 (1,747)\n481 (626)\n184 (100)\n\n\n\n\n\nOutside of Tacoma, overdose calls and accessibility were highest along the highway 7 and WA route 161 corridors. This is evidenced from the calls data themselves, but also the categorization of potential sites along these corridors into cluster 3."
  },
  {
    "objectID": "RCT.html#data-sources",
    "href": "RCT.html#data-sources",
    "title": "Using machine learning and optimization for resource planning: a case study in locating opioid use disorder treatment units",
    "section": "",
    "text": "2023 South Sound 911 Overdose Calls\nPierce County data on:\n\nFire stations\nLibraries\nSafe Parking sites\nRoads\nBus stops\n\nFixed MAT Locations"
  },
  {
    "objectID": "RCT.html#the-goods-methods",
    "href": "RCT.html#the-goods-methods",
    "title": "Using machine learning and optimization for resource planning: a case study in locating opioid use disorder treatment units",
    "section": "",
    "text": "Subject matter experts at Pierce County Human Services identified fire stations, libraries, and safe parking sites as potential partners for locating a mobile opioid use-disoreder van on the premesis. For each potential site i, I calculated the number of calls within 500, 1000, 2000, and 3000 meters. Because we want to prioritize potential sites with ease of access, I also calculate the distance to the closest bus stop and major road. I omitted sites in the City of Tacoma due to it’s high presence of fixed MAT location and greater access to services.\nAfter applying a standard scalar feature normalization I performed K-means clustering on the feature set. K-means applies equal weighting to all features. However, from the context it is clear that the proximity of overdose calls to potential sites is more relevant than access to the site alone. The multiple features for the number of calls within different specified radii of the site increase the relative importance of the number of calls nearby, as well as capture information from different ranges of distance to sites.\nThe site information, datasets, and clustering algorithm were packaged and ported into the Streamlit applicaiton. This allows the user to explore potential sites other than the pre-calculated sites at fire statations, libraries and safe parking.\nThis application alone was sufficient to help Pierce County Human Service’s program managers think through the implications of different site locations. However, it is unable to help researchers think through the implications of site selection on statistical power.\n\n\n\nThis methodology outlines a systematic approach to selecting experimental sites for a block-randomized trial in order to maximize statistical power while minimizing the risk of treatment spillover between sites. The statistical framework is based on the model of one treated unit per block with potentially multiple control units, following the approach of Pashley and Miratrix (2021). Power is calculated using pre-treatment outcome data to estimate within-block variances, and is directly linked to minimizing the variance of the treatment effect estimator. Key assumptions include homoskedasticity within blocks, independence between treatment and control units, and block-level heterogeneity in variance.\nTo operationalize site selection, a three-stage approach is developed. First, the method uses integer programming to select the largest possible number of sites that satisfy a minimum distance constraint, preventing spillover. Second, k-means clustering is applied to group selected sites into blocks with similar pre-treatment outcome values, minimizing within-block variance. Finally, statistical power is calculated based on this block structure using standard power formulas. This approach balances competing design goals: maximizing sample size, ensuring spatial independence, and minimizing outcome variance, ultimately yielding a more efficient and robust trial design.\n\n\n\n\n\n\nTechnical Details: Statistical Framework and Implementation\n\n\n\n\n\n\n\nThis document outlines the methodology used to select sites so as to maximize statistical power. This report begins by discussing the statistical model used in a block-randomized trial with exactly one treated unit per block and potentially multiple control units, where block sizes may be unbalanced. The approach leverages pre-treatment data to estimate variance components and follows the framework established by Pashley and Miratrix (2021) for calculating power. It discusses the potential of spillover effects in a spatial randomization, and develops a methodology to maximize statistical power while minimizing the potential for spillover.\n\n\n\nThe basic model for a block-randomized trial can be expressed as:\n\\[Y_{ik} = \\alpha_k + \\tau Z_{ik} + \\varepsilon_{ik}\\]\nWhere: - \\(Y_{ik}\\) is the outcome for unit \\(i\\) in block \\(k\\) - \\(\\alpha_k\\) is the block-specific fixed effect - \\(\\tau\\) is the treatment effect (our parameter of interest) - \\(Z_{ik}\\) is the treatment indicator (1 for treatment, 0 for control) - \\(\\varepsilon_{ik}\\) is the random error term\n\n\n\nFollowing Pashley and Miratrix (2021), the blocked estimator of the average treatment effect is:\n\\[\\hat{\\tau}_{(BK)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\hat{\\tau}_k\\]\nWhere: - \\(K\\) is the total number of blocks - \\(n_k\\) is the number of units in block \\(k\\) (including both treated and control) - \\(n = \\sum_{k=1}^{K} n_k\\) is the total sample size - \\(\\hat{\\tau}_k\\) is the estimated treatment effect within block \\(k\\)\nFor our design with exactly one treated unit per block, \\(\\hat{\\tau}_k\\) is:\n\\[\\hat{\\tau}_k = Y_k^T - \\bar{Y}_k^C\\]\nWhere \\(Y_k^T\\) is the outcome for the treated unit in block \\(k\\) and \\(\\bar{Y}_k^C\\) is the mean outcome for the control units in block \\(k\\).\n\n\n\nPashley and Miratrix (2021) provide the variance formula for the blocked estimator in their Equation (2). Adapting this for our specific case with one treated unit per block and assuming independence between treatment and control units within blocks:\n\\[Var(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\left(\\frac{n_k}{n}\\right)^2 Var(\\hat{\\tau}_k)\\]\nWithin each block, the variance of the block-specific treatment effect is:\n\\[Var(\\hat{\\tau}_k) = Var(Y_k^T) + Var(\\bar{Y}_k^C) = \\sigma_k^2 + \\frac{\\sigma_k^2}{n_k^C}\\]\nWhere: - \\(\\sigma_k^2\\) is the variance of outcomes within block \\(k\\) - \\(n_k^C = n_k - 1\\) is the number of control units in block \\(k\\)\nSimplifying:\n\\[Var(\\hat{\\tau}_k) = \\sigma_k^2\\left(1 + \\frac{1}{n_k^C}\\right) = \\sigma_k^2\\left(\\frac{n_k^C + 1}{n_k^C}\\right) = \\sigma_k^2\\left(\\frac{n_k}{n_k - 1}\\right)\\]\nTherefore, the variance of our treatment effect estimator is:\n\\[Var(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\left(\\frac{n_k}{n}\\right)^2 \\sigma_k^2\\left(\\frac{n_k}{n_k - 1}\\right) = \\sum_{k=1}^{K} \\frac{n_k^3}{n^2(n_k - 1)}\\sigma_k^2\\]\nFor this analysis, we make the following assumptions:\n\nHomoskedasticity within blocks: The variance of outcomes is constant within each block (i.e., treated and control units have the same variance within a block). This assumption is necessary as we have no pre-treatment data on treated outcomes with which to estimate the variance.\nIndependence: Treatment and control outcomes are independent within blocks, conditional on block membership.\nBlock-specific heterogeneity: The variance can differ across blocks (i.e., \\(\\sigma_k^2\\) can vary with \\(k\\)).\nTreatment affects means only: The treatment affects the mean of the outcome but not its variance.\nPre-treatment data is sufficient to estimate outcome variance post-treatment: The estimation of statistical power, as shown in in section 6, relies on an accurate estimation of the variance of our treatment effect.\n\n\n\n\nWe can leverage pre-treatment outcome data to estimate the variance components required for power analysis:\n\\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1} \\sum_{i=1}^{n_k} (Y_{ik,pre} - \\bar{Y}_{k,pre})^2\\]\nWhere: - \\(Y_{ik,pre}\\) is the pre-treatment outcome for unit \\(i\\) in block \\(k\\) - \\(\\bar{Y}_{k,pre}\\) is the mean pre-treatment outcome in block \\(k\\)\nThese estimates can then be substituted into the variance formula:\n\\[\\widehat{Var}(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\frac{n_k^3}{n^2(n_k - 1)}\\hat{\\sigma}_k^2\\]\n\n\n\nFor a given significance level \\(\\alpha\\) (typically 0.05) and a minimum detectable effect size \\(\\delta\\), the power of the test is:\n\\[Power = 1 - \\beta = \\Phi\\left(\\frac{|\\delta|}{\\sqrt{\\widehat{Var}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2}\\right)\\]\nWhere: - \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution - \\(z_{1-\\alpha/2}\\) is the critical value (e.g., 1.96 for \\(\\alpha = 0.05\\) with a two-sided test)\nAlternatively, to determine the required sample size for a desired power level \\(1-\\beta\\), we need to solve:\n\\[\\frac{|\\delta|}{\\sqrt{\\widehat{Var}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2} = z_{1-\\beta}\\]\nWhich gives:\n\\[\\widehat{Var}(\\hat{\\tau}_{(BK)}) = \\frac{\\delta^2}{(z_{1-\\alpha/2} + z_{1-\\beta})^2}\\]\nWe can then determine how many blocks or how many units per block would be needed to achieve this target variance.\n\n\n\nOur goal is to select sites to maximize statisticl power, while ensuring there is no spill over of treatment effects between sites. We can minimize the potential of spill over effects by ensuring all selected sites are at a minimum of X meters away from each other. The optimization progam is formed as follows:\nLet: - \\(S_{i,k}\\) be a 0/1 indicator for whether site \\(i\\) is in block \\(k\\) - \\((L_{i,1}, L_{i, 2})\\) be the coordinates of site \\(i\\) in a projection (EPSG:6559) such that the euclidian distance between two coordinates is meters - \\(D_{min}\\) be the minimum distance two sites must be apart for spill over to be neglegble\nThe objective function is:\n\\[ argmax_{S_{i,g}} \\Phi\\left(\\frac{|\\delta|}{\\sqrt{\\widehat{Var}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2}\\right)\\]\nSubject to:\n\\[ \\sum_{g} S_{i,g} \\leq 1 \\quad \\forall i \\]\n\\[\\sqrt{(L_{i,1}-L_{j,1})^2 + (L_{i,2}-L_{j,2})^2} \\geq (\\sum_{g} S_{i,g})(\\sum_{g} S_{j,g})D_{min} \\quad \\forall i, j \\quad if \\: i\\neq j   \\]\nHere the first constraint ensures each site can belong to only one block at most. The second constraint requires selected sites only to be at least \\(D_{min}\\) meters away from each other.\nThis optimiztion problem is constrained, highly non-linear, and is a mixed binary and continous problem. It not tractable with existing methods. However, examining the factors that influence the power calculation can yield insight into how to design an approach that may approximate site selection for maximal power. Specifically, power be maximized when the variance of the treatment effect estimator is minimized. Examining the formula for the treatment effect varience shows that it is minimized when sample sizes increase and when the within-block variances are minimized. Unfortunately, increasing sample sizes and minimizing within-block variance may not always be compatible. In section 9 below we detail an approach that weighs tradeoffs between maximizing sample size and minimizing within-block variance while ensure negligille spill over.\n\n\n\nOur approach to maximizing power while preventing spillover effects combines three stages:\n\n\nIn the first stage, we select the maximum number of sites that satisfy the distance constraint to prevent spillover effects while also filter to sites within pre-specified ranges of the outcome variable of interest. For example, we filter to all sites with more than 10 calls within 3,000 meters in a year. This filtering is aimed at reducing within-block variance prior to site selection, as the site selection process itself is ignorant of within-block variance. After prefiltering, we solve for the maximum number of sites.\nThis optimization is formulated as an integer programming problem:\nLet \\(S\\) be the set of all potential sites, and for each site \\(i \\in S\\): - \\(S_i\\) is a binary indicator variable where \\(S_i = 1\\) if site \\(i\\) is selected, and \\(S_i = 0\\) otherwise - \\(D_{i,j}\\) is a binary indicator where \\(D_{i,j} = 1\\) if the distance between sites \\(i\\) and \\(j\\) is greater than the minimum required distance (or if \\(i = j\\)), and \\(D_{i,j} = 0\\) otherwise. This reformulation removes the square-root constraint seen in section 7 and recasts the problem as a linear program.\nThe optimization problem is:\n\\[\\text{maximize } \\sum_{i \\in S} S_i\\]\nSubject to: \\[S_i + S_j \\leq D_{i,j} + 1 \\quad \\forall i,j \\in S\\]\nThis constraint ensures that if two sites \\(i\\) and \\(j\\) are both selected (\\(Z_i = Z_j = 1\\)), then they must be sufficiently far apart (\\(D_{i,j} = 1\\)). The constarint cannot be satisfied if both are selected and are too close.\n\n\n\nOnce we have selected the maximum number of eligible sites, we assign them to blocks to minimize within-block variance. For a design with exactly one treated unit per block, power is maximized when the within-block variance is minimized.\nWe use k-means clustering on the outcome variable to achieve this, where \\(k\\) equals the number of desired blocks:\n\nInitialize \\(k\\) centroids randomly from the outcome values of selected sites\nAssign each site to the nearest centroid based on the absolute difference in outcome values\nRecalculate centroids as the mean of outcome values in each cluster\nRepeat steps 2-3 until convergence\n\nFor one-dimensional clustering (i.e., clustering on a single outcome variable), the k-means loss function is:\n\\[\\sum_{k=1}^{K} \\sum_{i \\in C_k} (Y_i - \\mu_k)^2\\]\nWhere: - \\(C_k\\) is the set of sites assigned to block \\(k\\) - \\(Y_i\\) is the outcome value for site \\(i\\) - \\(\\mu_k\\) is the mean outcome value for block \\(k\\)\nThis loss function is directly related to minimizing the within-block variance, as the within-block variance for block \\(k\\) is:\n\\[\\sigma_k^2 = \\frac{1}{n_k-1} \\sum_{i \\in C_k} (Y_i - \\mu_k)^2\\]\nTherefore, minimizing the k-means loss function effectively minimizes the within-block variance, which maximizes power according to our variance formula in Section 4.\nAfter k-means clustering, we ensure each block has at least two sites (one treated, one control) by reassigning sites from larger blocks to smaller ones if necessary, based on outcome value similarity.\n\n\n\nWith the blocks established, we can calculate the power of our design using the formula from Section 6:\n\\[\\text{Power} = 1 - \\beta = \\Phi\\left(\\frac{|\\delta|}{\\sqrt{\\widehat{\\text{Var}}(\\hat{\\tau}_{(BK)})}} - z_{1-\\alpha/2}\\right)\\]\nWhere the variance of the treatment effect estimator is:\n\\[\\widehat{\\text{Var}}(\\hat{\\tau}_{(BK)}) = \\sum_{k=1}^{K} \\frac{n_k^3}{n^2(n_k - 1)}\\hat{\\sigma}_k^2\\]\nAnd \\(\\hat{\\sigma}_k^2\\) is estimated from the pre-treatment outcome data:\n\\[\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1} \\sum_{i=1}^{n_k} (Y_{ik} - \\bar{Y}_{k})^2\\]\nThe treatment effect size \\(\\delta\\) can be specified either as a fixed value or as a percentage of the average outcome across selected sites.\n\n\n\nKey considerations for implementation include:\n\nFiltering criteria: We apply filtering criteria to sites before stage 1 to ensure all candidate sites meet minimum requirements.\nMinimum selected sites: We require at least twice the number of blocks to be selected (to have at least one control unit per block). If fewer sites are selected, an error is raised.\nBlock balance: We enforce a minimum of two sites per block, which ensures we have at least one control unit per block.\nDistance calculation: Pairwise distances between sites are calculated using their geographic coordinates, typically in a projected coordinate system where Euclidean distance corresponds to meters.\nSignificance level: A standard significance level of α = 0.05 is typically used, corresponding to a critical value of \\(z_{1-\\alpha/2} = 1.96\\) for a two-sided test.\n\nBy following this three-stage approach, we maximize power by selecting the maximum number of sites subject to distance constraints and optimally assigning them to blocks to minimize within-block variance, while maintaining the ability to detect a specified treatment effect size with high probability.\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption\nRole in Power Formula\nRisk if Violated\n\n\n\n\nNormality of the estimator\nJustifies use of the standard normal distribution (\\(\\Phi\\)) for computing power.\nPower estimates may be inaccurate, especially if the number of blocks is small or outcome distributions are skewed.\n\n\nAccurate variance estimation\nRequired to plug into the denominator of the standardized effect size (\\(\\delta / \\sqrt{Var}\\)).\nUnder- or overestimation of variance leads to over- or underestimation of power.\n\n\nIndependence within blocks\nEnsures that variance components add properly; needed for block-specific variance formulas.\nCorrelated outcomes (e.g., due to interference or spillovers) can underestimate variance and inflate power.\n\n\nHomoskedasticity within blocks\nAssumes treated and control units within the same block have the same outcome variance.\nIf treatment changes variability, the variance formula for \\(\\hat{\\tau}_k\\) is invalid.\n\n\nConstant treatment effect\nAssumes a fixed treatment effect \\(\\delta\\) for the power calculation.\nIf effects vary across units/blocks, power calculations may not reflect the true ability to detect effects.\n\n\nCorrect \\(\\alpha\\) and \\(\\beta\\)\nDetermines critical values (\\(z_{1-\\alpha/2}\\), \\(z_{1-\\beta}\\)) used in the power formula.\nWrong values lead to incorrect inference thresholds (e.g., too lenient or too strict).\n\n\nNo covariate adjustment\nMatches the unadjusted variance formula derived from the block-only design.\nIf covariates are used in analysis but not in the power calculation, power may be underestimated.\n\n\n\n\n\n\nFor the results presented here, the parameters were:\n\nOutcome variable: Overdose calls within 3,000 meters\n\\(D_{min}\\): 6,000 meters\nFilter variable: Overdose calls within 3,000 meters\nFilter Threshold: Iterated over a minimum threshold starting at 0 and ending at 20 calls\nEffect size: 15% of the selected site mean\nNumber of blocks: 4\n\n\n\n\nFigure 1 below depicts the power calculations for selected sites based on the threshold minimum.\n\n\nWhile there is arrangement of sites with a power level greater than 80%, the analyses here represent a upper bound on the power calculations. This analysis is an upper bound on the power calculation because a number of key assumptions are likely violated. For instance, with only 4 blocks the normality of the estimator is unlikely to hold. Furthermore, the scenario where the power reached more than 80% had only 10 total sites selected. This means at most 3 sites per block were used to inform the estimate of the within-block variance.\nFigure 2 below plots the number of sites selected on the x-axis, the average within-block variance on the y-axsis, and a gradient and marker size value for the power with larger and redder markers indicating greater power.\n\n\nWhat is clear from figure 2 is that the arrangements with the highest power are driven almost entirely by reductions in the within-block variance. This combined with the low-sample sizes is concerning, since it could indicate our algorithm is overfitting. The low block sample sizes means our process for selecting blocks after site selection is likely underestimating the true within-block variance.\nThat being said, the sites that are selected do possess several desirable qualities. They are geographically distant from one another (see figure 3), have higher call volumnes than the average site, and have low-within block variance."
  },
  {
    "objectID": "RCT.html#next-steps",
    "href": "RCT.html#next-steps",
    "title": "Using machine learning and optimization for resource planning: a case study in locating opioid use disorder treatment units",
    "section": "",
    "text": "Containerize with Docker and deploy via AWS\nConsider alternative RCT design. If applicable, create synthetic data and run power calculations."
  },
  {
    "objectID": "RCT.html#references",
    "href": "RCT.html#references",
    "title": "Using machine learning and optimization for resource planning: a case study in locating opioid use disorder treatment units",
    "section": "",
    "text": "National Academies of Sciences, Engineering, and Medicine. (2018). Medication-Assisted Treatment for Opioid Use Disorder.\nPashley, N. E., & Miratrix, L. W. (2021). Insights on Variance Estimation for Blocked and Matched Pairs Designs. Journal of Educational and Behavioral Statistics, 46(3), 271-296."
  }
]